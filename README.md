# Python Pydantic to TypeScript LLM Converter

A GitHub Action that uses an LLM (Claude or GPT) to intelligently convert Python Pydantic models to TypeScript interfaces, maintaining styling and conventions.

## Features

- Compares base and new Python Pydantic models to identify changes
- Respects existing TypeScript conventions and styling
- Preserves comments and documentation
- Supports both Anthropic (Claude) and OpenAI (GPT) models
- Completely customizable with fine-grained control over the LLM parameters
- Accepts an _optional_ custom rule/message for extra generation instructions  
  _(Example: "Completely regenerate the .ts typescript file from the ground up from the new python file")_
- Supports _optional_ LangSmith tracing: if you provide a LangSmith API key, the action automatically logs LLM calls to LangSmith. You can optionally specify the project name; the run name is set to the base Python file’s name.
- Verbose logging: when enabled (default is `true`), the system message, user message, and LLM output are printed to the logs for debugging

## Usage

### Basic Usage

```yaml
name: Update TypeScript from Python Models

on:
  pull_request:
    paths:
      - 'src/models/*.py'
  workflow_dispatch:

jobs:
  update-typescript:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Checkout Base Branch
        uses: actions/checkout@v4
        with:
          ref: ${{ github.base_ref }}
          path: base-repo

      - name: Convert Python to TypeScript
        uses: yourusername/pydantic-to-typescript-action@v1
        with:
          base-python-file: 'base-repo/src/models/schema.py'
          new-python-file: 'src/models/schema.py'
          current-typescript-file: 'src/types/schema.ts'
          output-typescript-file: 'src/types/schema.ts'
          model-provider: 'anthropic'
          model-name: 'claude-3-haiku-20240307'
          anthropic-api-key: ${{ secrets.ANTHROPIC_API_KEY }}
          # Optional custom rule/message:
          # custom-prompt: "Completely regenerate the .ts typescript file from the ground up from the new python file"
          # Optional LangSmith tracing (if desired):
          # langsmith-api-key: ${{ secrets.LANGSMITH_API_KEY }}
          # langsmith-project: "my-custom-project"
          # Optional verbose logging:
          # verbose: "true"
```

### Workflow for Multiple Repositories

```yaml
name: Update Frontend TypeScript from Backend Python Models

on:
  pull_request:
    paths:
      - 'src/models/*.py'
  workflow_dispatch:

jobs:
  update-typescript:
    runs-on: ubuntu-latest
    steps:
      # Checkout the frontend repo where TS lives
      - name: Checkout Frontend Repository
        uses: actions/checkout@v4
        with:
          repository: yourusername/frontend-repo
          token: ${{ secrets.FRONTEND_REPO_PAT }}
          path: frontend-repo

      # Checkout the PR version of the backend repo
      - name: Checkout Backend Repository (PR)
        uses: actions/checkout@v4
        with:
          path: backend-repo
          fetch-depth: 0

      # Checkout the base branch of the backend repo
      - name: Checkout Backend Repository (Base)
        uses: actions/checkout@v4
        with:
          path: backend-base-repo
          ref: ${{ github.base_ref || 'main' }}
          fetch-depth: 0

      # Convert Python to TypeScript
      - name: Convert Python to TypeScript
        uses: yourusername/pydantic-to-typescript-action@v1
        with:
          base-python-file: 'backend-base-repo/src/models/schema.py'
          new-python-file: 'backend-repo/src/models/schema.py'
          current-typescript-file: 'frontend-repo/src/types/schema.ts'
          output-typescript-file: 'frontend-repo/src/types/schema.ts'
          model-provider: 'anthropic'
          anthropic-api-key: ${{ secrets.ANTHROPIC_API_KEY }}
          # Optional custom rule/message:
          # custom-prompt: "Add full documentation in pirate speak arrhhh"
          # Optional LangSmith tracing:
          # langsmith-api-key: ${{ secrets.LANGSMITH_API_KEY }}
          # langsmith-project: "my-custom-project"
          # Optional verbose logging:
          # verbose: "true"

      # Create a PR in the frontend repo
      - name: Create Pull Request
        uses: peter-evans/create-pull-request@v7
        with:
          token: ${{ secrets.FRONTEND_REPO_PAT }}
          path: frontend-repo
          commit-message: "Update TypeScript definitions via LLM"
          title: "Update TypeScript schema from Python changes"
          body: |
            This PR updates the TypeScript schema based on changes in the Python models.
            Generated by LLM from PR: ${{ github.event.pull_request.html_url || 'Manual trigger' }}
          branch: update-ts-schema
          branch-suffix: timestamp
```

## Inputs

| Input                     | Description                                                                                                                      | Required | Default                        |
|---------------------------|----------------------------------------------------------------------------------------------------------------------------------|----------|--------------------------------|
| `base-python-file`        | Path to the base Python Pydantic file                                                                                            | Yes      |                                |
| `new-python-file`         | Path to the new Python Pydantic file                                                                                             | Yes      |                                |
| `current-typescript-file` | Path to the current TypeScript file                                                                                              | Yes      |                                |
| `output-typescript-file`  | Path to the output TypeScript file                                                                                               | Yes      |                                |
| `model-provider`          | LLM provider to use (anthropic or openai)                                                                                       | No       | `anthropic`                    |
| `model-name`              | Specific model to use                                                                                                            | No       | `claude-3-haiku-20240307`      |
| `anthropic-api-key`       | Anthropic API key                                                                                                                | No       |                                |
| `openai-api-key`          | OpenAI API key                                                                                                                   | No       |                                |
| `temperature`             | Temperature for the LLM (0.0-1.0)                                                                                                | No       | `0.1`                          |
| `custom-prompt`           | Optional custom rule/message to be appended as additional instruction to the LLM.                                                | No       |                                |
| `langsmith-api-key`       | LangSmith API key for tracing LLM calls (optional)                                                                               | No       |                                |
| `langsmith-project`       | LangSmith project name for tracing; defaults to "pydantic-to-typescript-action" if not provided                                   | No       | `pydantic-to-typescript-action`|
| `verbose`                 | If `true`, prints the system message, user message, and LLM output to the logs (useful for debugging).                            | No       | `true`                         |

## LangSmith Tracing

If you provide a `langsmith-api-key`, the action automatically enables LangSmith tracing for your LLM calls. Tracing is activated by setting the following environment variables before executing the LLM:
- `LANGSMITH_TRACING` is set to `"true"`.
- `LANGSMITH_API_KEY` is set to your provided API key.
- `LANGSMITH_PROJECT` is set to your provided project name or defaults to `"pydantic-to-typescript-action"`.
- `LANGSMITH_RUN` is set to the base Python file’s name (used as the run name).

This integration allows you to capture and evaluate each LLM invocation without additional configuration steps.

## Verbose Logging

When the `verbose` option is enabled (default is `true`), the action prints:
- The system message sent to the LLM.
- The user message with dynamic content and any custom prompt.
- The output received from the LLM.

These logs can be extremely helpful for debugging or understanding the LLM's behavior.

## Development

### Prerequisites

- Node.js v16 or higher
- npm or yarn

### Setup

1. Clone the repository
2. Install dependencies
   ```
   npm install
   ```
3. Build the project
   ```
   npm run build
   ```

### Testing

```
npm test
```

## License

MIT
